{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from missingpy import MissForest\n",
    "import multiprocessing\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This goes through the first step in collecting and combining data\n",
    "Note, some of the packages were too large to load on Github, so they will not be included in the folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in processed training data\n",
    "print('Read in processed training data')\n",
    "#proc_train_df = pd.read_csv('training_data_woEncoding_threshold_0.8.csv')\n",
    "\n",
    "# Read in unprocessed training data\n",
    "print('Read in unprocessed training data')\n",
    "weather_train_fn = 'ashrae-energy-prediction/weather_train.csv'\n",
    "weather_train_df = pd.read_csv(weather_train_fn)\n",
    "train_fn = 'ashrae-energy-prediction/train.csv'\n",
    "train_df = pd.read_csv(train_fn)\n",
    "\n",
    "building_fn ='ashrae-energy-prediction/building_metadata.csv' \n",
    "building_df = pd.read_csv(building_fn)\n",
    "\n",
    "# Combine data\n",
    "train_df = pd.merge(building_df, train_df, on='building_id')\n",
    "train_df = pd.merge(weather_train_df, train_df, on=['site_id', 'timestamp'])\n",
    "\n",
    "# Feature Engineering\n",
    "datetimeobject = train_df.timestamp.apply(lambda x:datetime.datetime.strptime(x, \"%Y-%m-%d %H:%M:%S\"))\n",
    "train_df['WeatherYear'] = [i.year for i in datetimeobject]\n",
    "train_df['WeatherMonth'] = [i.month for i in datetimeobject]\n",
    "train_df['WeatherDay'] = [i.day for i in datetimeobject]\n",
    "train_df['WeatherHour'] = [i.hour for i in datetimeobject]\n",
    "train_df['WeatherWeekend'] = [1 if i > 4 else 0  for i in datetimeobject.weekday()]\n",
    "train_df = train_df.drop(['timestamp'], axis=1)\n",
    "\n",
    "print(\"Finished feature engineering.\")\n",
    "train_df.to_csv('train_df_step1.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process the rest of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in train data    \n",
    "train_df = pd.read_csv('train_df_step1.csv', low_memory=True)\n",
    "nearlyComplete = train_df.isnull().sum(1) < 5\n",
    "train_df = train_df[nearlyComplete == True]\n",
    "train_df = train_df[train_df['meter_reading'] != np.inf]\n",
    "train_df = train_df.sample(frac=0.1, random_state = 0)\n",
    "\n",
    "# Assign random variables needed\n",
    "print('Assign random variables needed')\n",
    "features = ['air_temperature', 'cloud_coverage', 'dew_temperature', 'precip_depth_1_hr', 'wind_speed',\n",
    "            'wind_direction', 'sea_level_pressure', 'square_feet', 'year_built', \n",
    "            'WeatherYear', 'WeatherMonth', 'WeatherDay', 'WeatherHour']\n",
    "cat_vars = ['site_id', 'primary_use', 'building_id', 'meter']\n",
    "\n",
    "# Make test data\n",
    "print('Make test data')\n",
    "weather_test_fn = 'ashrae-energy-prediction/weather_test.csv'\n",
    "weather_test_df = pd.read_csv(weather_test_fn)\n",
    "test_fn = 'ashrae-energy-prediction/test.csv'\n",
    "test_df = pd.read_csv(test_fn)\n",
    "building_fn ='ashrae-energy-prediction/building_metadata.csv' \n",
    "building_df = pd.read_csv(building_fn)\n",
    "test_df = pd.merge(building_df, test_df, on='building_id')\n",
    "test_df = pd.merge(weather_test_df, test_df, on=['site_id', 'timestamp'])\n",
    "\n",
    "# Feature engineering\n",
    "print('Feature engineering')\n",
    "datetimeobject = test_df.timestamp.apply(lambda x:datetime.datetime.strptime(x, \"%Y-%m-%d %H:%M:%S\"))\n",
    "test_df['WeatherWeekend'] = [1 if i.weekday() > 4 else 0  for i in datetimeobject]\n",
    "test_df['WeatherYear'] = [i.year for i in datetimeobject]\n",
    "test_df['WeatherMonth'] = [i.month for i in datetimeobject]\n",
    "test_df['WeatherDay'] = [i.day for i in datetimeobject]\n",
    "test_df['WeatherHour'] = [i.hour for i in datetimeobject]\n",
    "test_df = test_df.drop(['timestamp'], axis=1)\n",
    "\n",
    "# Drop columns not found in processed train\n",
    "print('Drop columns not found in processed train')\n",
    "dropCols = [col for col in test_df.columns if col not in proc_train_df.columns]\n",
    "dropCols.remove('row_id')\n",
    "test_df = test_df.drop(dropCols, axis=1)\n",
    "dropCols = [col for col in train_df.columns if col not in proc_train_df.columns]\n",
    "train_df = train_df.drop(dropCols, axis=1)\n",
    "\n",
    "# Combine Data\n",
    "print('Combine Data')\n",
    "comb_df = pd.concat([train_df, test_df])\n",
    "del building_df\n",
    "del weather_test_df\n",
    "\n",
    "# Write to a file\n",
    "comb_df.to_csv('combined.csv', chunksize=1000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This section goes through the subsequent preprocessing steps of the entire data: normalization and imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read in combined data\n",
      "Assign random variables needed\n",
      "Fix Primary Use\n"
     ]
    }
   ],
   "source": [
    "# Read in combined data\n",
    "print('Read in combined data')\n",
    "comb_df = pd.read_csv('combined.csv')\n",
    "\n",
    "# Assign random variables needed\n",
    "print('Assign random variables needed')\n",
    "features = ['air_temperature', 'cloud_coverage', 'dew_temperature', 'precip_depth_1_hr', 'wind_speed',\n",
    "            'wind_direction', 'sea_level_pressure', 'square_feet', 'year_built', \n",
    "            'WeatherYear', 'WeatherMonth', 'WeatherDay', 'WeatherHour']\n",
    "cat_vars = ['site_id', 'primary_use', 'building_id', 'meter']\n",
    "\n",
    "# Fix Primary Use\n",
    "print('Fix Primary Use')\n",
    "rowVals = list(comb_df['primary_use'])\n",
    "newRowVals = [c.replace('/', '') for c in rowVals]\n",
    "comb_df['primary_use'] = newRowVals\n",
    "use_encoder = LabelEncoder()\n",
    "comb_df['primary_use'] = use_encoder.fit_transform(comb_df['primary_use'])\n",
    "\n",
    "# Scramble rows\n",
    "comb_df = comb_df.sample(frac=1, random_state=100)\n",
    "\n",
    "print('Imputation steps')\n",
    "# Find rows not missing anything and use to impute\n",
    "complete = comb_df.isnull().sum(1) < 2\n",
    "\n",
    "# Impute set up variables\n",
    "replaceCols = features + cat_vars\n",
    "cat_idx = [i for i, e in enumerate(replaceCols) if e in cat_vars]\n",
    "\n",
    "# Impute\n",
    "imp = MissForest(max_iter=3)\n",
    "imp.fit(comb_df[replaceCols], cat_vars=cat_idx)\n",
    "print('Finished impute fit.')\n",
    "\n",
    "def parallelize_dataframe(df, func, n_cores=4):\n",
    "    df_split = np.array_split(df, n_cores)\n",
    "    pool = multiprocessing.Pool(n_cores)\n",
    "    df = pd.concat(pool.map(func, df_split))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return df\n",
    "\n",
    "def func(df):\n",
    "    values = imp.transform(df[replaceCols])\n",
    "    df[replaceCols] = values\n",
    "    return df\n",
    "\n",
    "comb_df = parallelize_dataframe(comb_df, func, n_cores=4)\n",
    "\n",
    "# Reverse encode\n",
    "print('Reverse Encode')\n",
    "useList = [int(i) for i in comb_df['primary_use']]\n",
    "comb_df['primary_use'] = use_encoder.inverse_transform(useList)\n",
    "\n",
    "# Print output\n",
    "print('Finished impute transform.')\n",
    "comb_df.to_csv('imputed.csv')\n",
    "\n",
    "# Normalize\n",
    "print('Normalize')\n",
    "min_max_scaler = MinMaxScaler()\n",
    "values = min_max_scaler.fit_transform(comb_df[features])\n",
    "comb_df[features] = values\n",
    "print('Finished normalization.')\n",
    "comb_df.to_csv('normalized.csv', chunksize=1000)\n",
    "\n",
    "# Separate Train from test data\n",
    "print('Separate Train from test data and finalize documents')\n",
    "test_df = comb_df.loc[comb_df['meter_reading'].isna()]\n",
    "train_df = comb_df.loc[comb_df['row_id'].isna()]\n",
    "test_df = test_df.drop(['meter_reading'], axis = 1)\n",
    "train_df = train_df.drop(['row_id'], axis = 1)\n",
    "\n",
    "# Print to file\n",
    "train_df.to_csv('train.csv', chunksize=1000)\n",
    "test_df.to_csv('test.csv', chunksize=1000)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
